{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install db-sqlite3\n",
    "# !pip install random\n",
    "# !pip install pandas\n",
    "# !pip install csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import random\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import dask.dataframe as dd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the data's columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create 4 different lists: fruits, colors, prices and a list of Consecutive numbers which will be later defined as the index column, each one of the required lists will contain 10^6 ranodmize variables in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_col = list(range(1, 1000001))\n",
    "# index_col # uncomment to view the coloumn's content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits_list = ['Orange', 'Grape', 'Apple', 'Banana', 'Pineapple', 'Avocado']\n",
    "fruits_col = random.choices(fruits_list, k=1000000)\n",
    "# fruits_col # uncommment to view the column's content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_list = ['Red', 'Green', 'Yellow', 'Blue']\n",
    "colors_col = random.choices(colors_list, k=1000000)\n",
    "# colors_col # uncomment to view to column's content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_list = list(range(10,101))\n",
    "prices_col = random.choices(prices_list, k=1000000)\n",
    "# prices_col # uncommment to view the column's content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Data Frame and the CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a DataFrame using the 4 lists we created above and then create a csv file called 'my_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"C:\\\\Users\\\\Yam Daniel\\\\OneDrive\\\\Machine Learning & Data Science\\\\Big Data Platform\\\\HW\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(index_col, fruits_col, colors_col, prices_col)),\n",
    "                  columns=['id', 'Fruit', 'Color', 'Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{folder_path}/my_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SQL Data Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - CSV & SQL\n",
    "### <ins>Subtask 1</ins>\n",
    "Create a pytohn code to create SQLite database 'mydb.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect(f'{folder_path}/mydb.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a table 'mydata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('''CREATE TABLE IF NOT EXISTS mydata(\n",
    "                    id integer PRIMARY KEY,\n",
    "                    Fruit text NOT NULL,\n",
    "                    Color text NOT NULL,\n",
    "                    Price integer NOT NULL)''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Subtask 2</ins>\n",
    "Now we will read the csv file that was created earlier and converte it into a SQL database's structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{folder_path}/my_data.csv','r') as csv_file:\n",
    "    root = csv.DictReader(csv_file)\n",
    "    db_format = [(i['id'], i['Fruit'], i['Color'], i['Price']) for i in root]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the csv's content into the SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.executemany(\"INSERT INTO mydata (id, Fruit, Color, Price) VALUES (?, ?, ?, ?);\", db_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Subtask 3</ins>\n",
    "We will use 2 different statments with different logics to retrieve different rows from our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st statement - retrieve all of the items which priced higher than 46\n",
    "                # sort by Fruit, then by Color and then by price\n",
    "cursor.execute('''\n",
    "                SELECT * \n",
    "                FROM mydata\n",
    "                WHERE Price>46\n",
    "                ORDER BY Fruit, Color, Price\n",
    "                  ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd statement - display how many red fruits there are in the dataset\n",
    "cursor.execute('''\n",
    "                SELECT Fruit, COUNT(Fruit) as Fruit_Count\n",
    "                FROM mydata\n",
    "                WHERE Color == \"Red\" \n",
    "                GROUP BY FRUIT\n",
    "                ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection & Predicate\n",
    "<ins>Predicate Part</ins>\n",
    "\n",
    "The predicate part of the statement is were we difine the 'WHERE' attribute. In statement 1 we selected all rows with a price higher than 46, where all rows with a price higher then 46 are evaluated as TRUE, and all others are evaluted as False. Using the 'WHERE' attribute will allow the user to display only rows with the same logic thet was choosen.\n",
    "\n",
    "<ins>Projection Part</ins>\n",
    "\n",
    "The projection part of the statement is were we defined the 'SELECT' attribute. Using the projection operation, the user could define how many attributes from the dataset will be displayed. We can see that in statement 1 all of the columns in the dataset were displayed, while in statment 2 only the rows under 'Fruit' column were shown, aside with another column that counts how many records are in the dataset grouped by the 'Fruit' column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - CSV & Parquet\n",
    "### <ins>Subtask 1</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rows(df_path):\n",
    "    # if the csv exists then read it\n",
    "    if os.path.isfile(df_path):\n",
    "        df = pd.read_csv(df_path)\n",
    "        return len(df)\n",
    "    else:\n",
    "        print (f'There is not such file {df_path}')\n",
    "        \n",
    "count_rows(f\"{folder_path}/my_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Subtask 2</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_data = pd.read_csv(f\"{folder_path}/my_data.csv\")\n",
    "table = pa.Table.from_pandas(pandas_data)\n",
    "pq.write_table(table, \n",
    "               f\"{folder_path}/mydatapyarrow.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Subtask 3</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dask_data = dd.read_csv(f\"{folder_path}/my_data.csv\")\n",
    "dask_data.to_parquet(f'{folder_path}/mydatadask.parquet') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Subtask4</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_data.to_parquet(f'{folder_path}/mydatapandas.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Subtask 5</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learn that the parquet file that was generated using Dask operator was generated differently than the files that were generated using Pandas and PyArrow. Moreover, We learn that Pandas acyually leverages the PyArrow library to write parquet files, so we can assume the process is very similar between these 2 libraries.\n",
    "\n",
    "We believe the main difference between Dask and Pandas & pyArrow libraries is the fact that Dask library enables the user to perform parallel computations, by splitting the data into different partitions that would bo stored inside one folder (i.e. Dask allows a single Pandas DataFrame generated from the original Dataframe to be worked on in parallel by multiple hosts). On the other hand Pandas and PyArrow generate only one parquet file (with whole dataset) to work with each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Split CSV files\n",
    "### <ins>Subtask 1</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_size(df):\n",
    "    df_size = os.path.getsize(df)\n",
    "    middle = df_size//2\n",
    "    print(f\"DataFrame size in bytes is {df_size}\\nMiddle is {middle}\")\n",
    "    return middle\n",
    "    \n",
    "middle = get_df_size(f\"{folder_path}/my_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Subtask 2</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_chunk(df_path: str, chunk_size: int):    \n",
    "    df = open(df_path, \"rb\")\n",
    "    df_str = df.read(chunk_size).decode(encoding='utf-8')\n",
    "    return len(df_str.split(\"\\r\\n\"))\n",
    "\n",
    "def last_chunk(df_path: str, chunk_size: int):\n",
    "    df = open(df_path, \"rb\")\n",
    "    df.seek(chunk_size)\n",
    "    df_str = df.read().decode(encoding='utf-8')\n",
    "    return len(df_str.split(\"\\r\\n\"))\n",
    "    \n",
    "\n",
    "first = first_chunk(f\"{folder_path}/my_data.csv\", middle)\n",
    "\n",
    "last = last_chunk(f\"{folder_path}/my_data.csv\", middle)\n",
    "\n",
    "print(f'Total rows {first+last}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Subtask 3</ins>\n",
    "\n",
    "We can see that the total number of lines from the first chunk and the second chunk is bigger by 3 rows in total than the total number of rows that was calculated in subtask 1.\n",
    "\n",
    "One explaination for the difference between the two methods could be the fact that when the csv file was converted into bytes, it also included the header row and the blank row at the end of the csv file, resulting in more rows then there actually are.\n",
    "Another explaination for the difference could be that splitting the csv file into half can eventually cause reading the same row twice, because of the fact that the middle point might be at the middle of a row, thus it will be splitted into 2 halfs, therfore create more rows in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_chunk(df_path: str, chunk_size: int):\n",
    "    df = open(df_path, \"rb\")\n",
    "    df_str = df.read(chunk_size).decode(encoding='utf-8')\n",
    "    while df_str[-1] != '\\r':\n",
    "        df = open(df_path, \"rb\")\n",
    "        chunk_size += 1\n",
    "        df_str = df.read(chunk_size).decode(encoding='utf-8')\n",
    "    df = open(df_path, \"rb\")\n",
    "    chunk_size += 1\n",
    "    df_str = df.read(chunk_size).decode(encoding='utf-8')\n",
    "    df_list = df_str.split('\\r\\n')\n",
    "    df_list = df_list[1:]\n",
    "    if df_str.split('\\r\\n')[-1] == '':\n",
    "        df_list = df_list[:-1]\n",
    "    return len(df_list), chunk_size\n",
    "\n",
    "def last_chunk(df_path: str, chunk_size: int):\n",
    "    df = open(df_path, \"rb\")\n",
    "    df.seek(chunk_size)\n",
    "    df_str = df.read().decode(encoding='utf-8')\n",
    "    df_list = df_str.split(\"\\r\\n\")\n",
    "    if df_list[-1] == '':\n",
    "        df_list = df_list[:-1]\n",
    "    if df_list[0] == '':\n",
    "        df_list = df_list[1:]\n",
    "    return len(df_list)\n",
    "\n",
    "    \n",
    "first, chunk_modified = first_chunk(f\"{folder_path}/my_data.csv\", chunk_size=middle)\n",
    "last = last_chunk(f\"{folder_path}/my_data.csv\", chunk_size=chunk_modified)\n",
    "print(f'1st chunk value - {first}')\n",
    "print(f'2nd chunk value - {last}')\n",
    "print(f'Total number of rows after modification - {first+last}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Subtask 4</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rows_using_chunks(df_path: str, chunk_size: int):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----------\n",
    "    This function get a csv file's path and a desired chunk size\n",
    "    and returns the number of rows in the csv\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    df_path (str) = path to a csv file\n",
    "    chunk_size (int) = size of the chunk in MB (i.e. 16MB -> chunk_size=16)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    total rows in the data set and a dictionary that tells how many bytes where in each chunk\n",
    "    \"\"\"\n",
    "    chunk_size = chunk_size*1000000 # real chunk size\n",
    "    \n",
    "    df_size = os.path.getsize(df_path) # size of the df in bytes\n",
    "    \n",
    "    total_chunks = math.ceil(df_size/(chunk_size)) # number of chunks\n",
    "    \n",
    "    cum_chunk = 0 # cummulative variable which defines the current byte\n",
    "    rows_dict = {} # dictionary that would display each chunk and its value (in bytes)\n",
    "    \n",
    "    # if there is only one chunk, then split it and count the values (excluding the header row)\n",
    "    if total_chunks == 1:\n",
    "        df = open(df_path, \"rb\")\n",
    "        df_str = df.read(chunk_size).decode(encoding='utf-8')\n",
    "        \n",
    "        # check if the last element in the list is a valid row (an empty row is not valid either)\n",
    "        if len(df_str.split(\"\\r\\n\")[-1].split(',')) != 4:\n",
    "            return len(df_str.split(\"\\r\\n\")) -2\n",
    "        else:\n",
    "            return len(df_str.split(\"\\r\\n\")) - 1\n",
    "        \n",
    "    # if there are multiple chunks then start the process\n",
    "    else:\n",
    "        for i in range(total_chunks):\n",
    "            # for each chunk\n",
    "            if i+1 < total_chunks:\n",
    "                \n",
    "                # if this is the first chunk so use the first_chunk function\n",
    "                if cum_chunk == 0: \n",
    "                    first_value, cum_chunk = first_chunk(df_path, chunk_size)\n",
    "                    rows_dict['first'] = first_value\n",
    "                    \n",
    "                else:\n",
    "                    df = open(df_path, \"rb\")\n",
    "                    df.seek(cum_chunk) # look for the current byte and read from it and on\n",
    "                    df_str = df.read(chunk_size).decode(encoding='utf-8') # at first read the number of bytes defined in chunk_size\n",
    "                    \n",
    "                    # if the last byte is not the end of the row then add 1 to the cummulative chunk_size\n",
    "                    while df_str[-1] != '\\r':\n",
    "                        df = open(df_path, \"rb\")\n",
    "                        chunk_size += 1 \n",
    "                        df.seek(cum_chunk)\n",
    "                        df_str = df.read(chunk_size).decode(encoding='utf-8')\n",
    "                    df = open(df_path, \"rb\")\n",
    "                    chunk_size += 1\n",
    "                    df.seek(cum_chunk)\n",
    "                    df_str = df.read(chunk_size).decode(encoding='utf-8')\n",
    "                    df_list = df_str.split('\\r\\n') \n",
    "                    if df_str.split('\\r\\n')[-1] == '':\n",
    "                        df_list = df_list[:-1]\n",
    "                    cum_chunk += chunk_size\n",
    "                    rows_dict[i] = len(df_list)\n",
    "                    \n",
    "            # if it is not the first chunk and not a middle one then it is the last\n",
    "            else:\n",
    "                last_value = last_chunk(df_path, cum_chunk)\n",
    "                rows_dict['last'] = last_value\n",
    "    \n",
    "    # define a variable that will count the values of the dictionary\n",
    "    rows_sum = 0\n",
    "    for key,value in rows_dict.items():\n",
    "        rows_sum += value\n",
    "    \n",
    "    print(f\"There are {rows_sum} rows in the csv\")\n",
    "    return rows_dict\n",
    "\n",
    "count_rows_using_chunks(f\"{folder_path}/my_data.csv\", chunk_size=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
